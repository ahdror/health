import streamlit as st
import requests
import json
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.language_models.llms import LLM
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.callbacks.manager import CallbackManagerForLLMRun
from typing import Any, List, Optional

st.set_page_config(page_title="í—¬ìŠ¤ íŠ¸ë ˆì´ë„ˆ", page_icon="ğŸ§Š")
st.title("ğŸ§Š í—¬ìŠ¤ íŠ¸ë ˆì´ë„ˆ")

st.session_state["messages"] = []
st.session_state["store"] = dict()


# Initialize session state
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# Store chat history in session state
if "store" not in st.session_state:
    st.session_state["store"] = dict()

# Sidebar for session ID, API keys and reset buttons
with st.sidebar:
    session_id = st.text_input("ì ‘ì†ìëª…", value="ì†ë‹˜")

    
    if st.button("ëŒ€í™” í™”ë©´ ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.rerun()
    if st.button("ëŒ€í™” ì €ì¥ ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state["store"] = dict()
        st.rerun()

#ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
def print_messages():
    ## ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ    
    if "messages" in st.session_state and len(st.session_state["messages"]) > 0:
        for chat_message in st.session_state["messages"]:
            st.chat_message(chat_message.role).write(chat_message.content)

# Print previous chat messages
print_messages()

# Function to retrieve or create a chat history object associated with the session id
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    if session_ids not in st.session_state["store"]:
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]

# Define the custom LLM class
class LlmClovaStudio(LLM):
    host: str
    api_key: str
    api_key_primary_val: str
    request_id: str

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.host = kwargs.get('host')
        self.api_key = kwargs.get('api_key')
        self.api_key_primary_val = kwargs.get('api_key_primary_val')
        self.request_id = kwargs.get('request_id')

    @property
    def _llm_type(self) -> str:
        return "custom"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None
    ) -> str:
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")

        headers = {
            "X-NCP-CLOVASTUDIO-API-KEY": self.api_key,
            "X-NCP-APIGW-API-KEY": self.api_key_primary_val,
            "X-NCP-CLOVASTUDIO-REQUEST-ID": self.request_id,
            "Content-Type": "application/json; charset=utf-8",
            "Accept": "text/event-stream"
        }

        system_prompt = '''\
        # ì§€ì‹œì‚¬í•­ 
        - ë‹¹ì‹ ì€ í—¬ìŠ¤ íŠ¸ë ˆì´ë„ˆ ì…ë‹ˆë‹¤.
        - ì„±ë³„,ëª¸ë¬´ê²Œ,í‚¤,ìš´ë™ë°©ë²•ì˜ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì…ë ¥í•˜ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.
        - ìš´ë™ ë° ì‹ë‹¨ì— ê´€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´, ì •ë³´ë¥¼ ë¬´ì‹œí•˜ê³  ë‹µë³€í•˜ì„¸ìš”
        - ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
        - ì •ë³´ì™€ ê´€ê³„ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´, ë‹¹ì‹ ì´ ì•„ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        '''
        preset_text = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]

        request_data = {
            "messages": preset_text,
            "topP": 0.9,
            "topK": 0,
            "maxTokens": 1024,
            "temperature": 0.1,
            "repeatPenalty": 1.2,
            "stopBefore": [],
            "includeAiFilters": False
        }

        response = requests.post(
            self.host + "/testapp/v1/chat-completions/HCX-DASH-001",
            headers=headers,
            json=request_data,
            stream=True
        )

        last_data_content = ""
        for line in response.iter_lines():
            if line:
                decoded_line = line.decode("utf-8")
                if '"data":"[DONE]"' in decoded_line:
                    break
                if decoded_line.startswith("data:"):
                    last_data_content = json.loads(decoded_line[5:])["message"]["content"]

        return last_data_content

# Process user input and make a call to the LLM if input is provided
if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    st.chat_message("user").write(f"{user_input}")
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))


    # Instantiate custom LLM with user-provided API keys
    llm = LlmClovaStudio(
        host='https://clovastudio.stream.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiYxXVubGNTI5Vvj3+7OKX/EUwHeJtv0sO1VC74rpQaK2O',
        api_key_primary_val='JaWxqB0Ub0fKPzDXwPhDXWlLv4OfHRG3UTmp8NRr',
        request_id='37d72c32-2228-47b4-86c4-56ae0168c473'
    )

    # Create prompt
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ì„¤ëª…ì„ ìì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}"),
        ]
    )
    chain = prompt | llm

    chain_with_memory = (
        RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="question",
            history_messages_key="history"
        )
    )

    response_content = chain_with_memory.invoke(
        {"question": user_input},
        config={"configurable": {"session_id": session_id}}
    )

    #assistant ì‘ë‹µ í•„ìš”ì—†ëŠ” ë¬¸ì ì œê±°
    cleaned_response = response_content.replace('Human: ', '').replace('AI: ', '').strip()
    cleaned_response = cleaned_response.replace('System: ', '').strip()

    # Add the assistant's response to session_state
    st.session_state["messages"].append(
        ChatMessage(role="assistant", content=cleaned_response)
    )

    # assistant ì‘ë‹µ ì¶œë ¥
    st.chat_message("assistant").write(cleaned_response)
